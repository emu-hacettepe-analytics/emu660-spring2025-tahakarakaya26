---
title: "Assignment 1"
---

My first assignment has three parts.

## a) Discussion on Data Science and Industrial Engineering - Kerem Demirtas & Erdi Dasdemir

You can access the related discussion through this link. (<https://www.youtube.com/watch?v=cUPtQEyaswM>)

### The Integration of Industrial Engineering and Data Science: A Key to Smarter Decision-Making

The integration of industrial engineering (IE) and data science (DS) is becoming increasingly important in today's business world. This discussion particularly focused on how industrial engineers can enhance their analytical skills and transition into the field of data science. The relationship between traditional optimization techniques and machine learning was explored, highlighting that deterministic modeling methods may fall short in highly variable and uncertain environments. As a result, data-driven approaches such as event-based simulation, probabilistic modeling, and statistical learning are gaining more significance. It was also emphasized that essential competencies for industrial engineers in this field include proficiency in SQL, Python, and statistical analysis tools. Data-driven decision support systems have the potential to solve traditional industrial engineering problems---such as supply chain management, logistics, and inventory optimization---much more efficiently.

Moreover, it was underscored that theoretical knowledge alone is not sufficient; working on real-world problems is essential. Platforms like Kaggle provide valuable opportunities to develop hands-on projects, connect with professionals in the field, and focus on industry-specific challenges, all of which are crucial for career development. For instance, an industrial engineer applying data science to demand forecasting, production planning, or operational efficiency can gain a significant competitive edge. Many conventional engineering problems can now be enhanced using machine learning-based forecasting models, time series analysis, and big data solutions, ultimately leading to smarter and more optimized processes. Lastly, the discussion emphasized that industrial engineers should not only focus on developing models but also understand business processes, accurately define problems, and implement strategic, optimized solutions.

### Personal Takeaways and Career Development

As a recent graduate working in planning operations, I have had the opportunity to closely observe the growing importance of integrating data science into business processes. Through this experience, I have realized the critical role of data analytics and automation tools in strengthening decision support systems. To enhance efficiency in reporting processes, I have been improving my automation skills while also leveraging predictive models to support strategic decision-making. Moving forward, I aim to further develop my expertise in statistical analysis, machine learning, and optimization techniques to create added value in business processes.

## b) Statistical Summaries with Custom Functions and Iteration Methods in mtcars dataset:

### import data

```{r}
data(mtcars)
mtcars
```

### discover the mtcars dataset

```{r}
str(mtcars)
summary(mtcars)
```

### compute_stats

```{r}
compute_stats <- function(x) {
  stats <- list(
    mean = mean(x, na.rm = TRUE),
    median = median(x, na.rm = TRUE),
    variance = var(x, na.rm = TRUE),
    iqr = IQR(x, na.rm = TRUE),
    min = min(x, na.rm = TRUE),
    max = max(x, na.rm = TRUE)
  )
  return(stats)
}
```

### df of compute_stats

```{r}
compute_stats <- function(x) {
  stats <- list(
    mean = mean(x, na.rm = TRUE),
    median = median(x, na.rm = TRUE),
    variance = var(x, na.rm = TRUE),
    iqr = IQR(x, na.rm = TRUE),
    min = min(x, na.rm = TRUE),
    max = max(x, na.rm = TRUE)
  )
  return(stats)
}

stats_df <- data.frame(Column = character(),
                       Mean = numeric(),
                       Median = numeric(),
                       Variance = numeric(),
                       IQR = numeric(),
                       Min = numeric(),
                       Max = numeric(),
                       stringsAsFactors = FALSE)

for (col in names(mtcars)) {
  
  if (is.numeric(mtcars[[col]])) {
    
    stats <- compute_stats(mtcars[[col]])
    
    new_row <- data.frame(Column = col,
                          Mean = stats$mean,
                          Median = stats$median,
                          Variance = stats$variance,
                          IQR = stats$iqr,
                          Min = stats$min,
                          Max = stats$max,
                          stringsAsFactors = FALSE)
    
    stats_df <- rbind(stats_df, new_row)
  }
}

library(knitr)
kable(stats_df, caption = "Stats Df")

```

### with sapply

```{r}
data(mtcars)
compute_stats <- function(x) {
  c(Mean = mean(x, na.rm = TRUE),
    Median = median(x, na.rm = TRUE),
    Variance = var(x, na.rm = TRUE),
    IQR = IQR(x, na.rm = TRUE),
    Min = min(x, na.rm = TRUE),
    Max = max(x, na.rm = TRUE))
}
stats_matrix <- sapply(mtcars, compute_stats)
stats_df <- as.data.frame(t(stats_matrix))
stats_df <- cbind(Column = rownames(stats_df), stats_df)
rownames(stats_df) <- NULL

library(knitr)
kable(stats_df, caption = "Stats Df")

```

## c) dslabs, missing values

```{r}
library(dslabs)
data(na_example)
na_example
```

### how many? where?

```{r}
na_count <- sum(is.na(na_example))
cat("Number of total NA:", na_count, "\n")

na_indices <- which(is.na(na_example))
cat("Indices of NA values:", na_indices, "\n")
```

### mean and std calculations except NA values

```{r}
data("na_example")
mean_original <- mean(na_example, na.rm = TRUE)
sd_original <- sd(na_example, na.rm = TRUE)

cat("Data except NA's - Mean:", mean_original, "\n")
cat("Data except NA's - Standard Deviation:", sd_original, "\n")
```

### handling missing values, version 1, replacing with median

```{r}
data("na_example")
median_value <- median(na_example, na.rm = TRUE)
na_median_filled <- ifelse(is.na(na_example), median_value, na_example)

mean_median <- mean(na_median_filled)
sd_median <- sd(na_median_filled)

cat("data of replaced with median --- mean:", mean_median, "\n")
cat("data of replaced with median --- std:", sd_median, "\n")
```

### handling missing values, version 2, replacing with random values

```{r}
data("na_example")
na_fixed_filled <- ifelse(is.na(na_example), 5, na_example)

mean_fixed <- mean(na_fixed_filled)
sd_fixed <- sd(na_fixed_filled)

cat("data of replaced with fixed value (5) --- mean:", mean_fixed, "\n")
cat("data of replaced with fixed value (5) --- std:", sd_fixed, "\n")
```

### comparing results

```{r}
comparison_df <- data.frame(
  Dataset = c("Original", "Median Filled", "Fixed Value (5)"),
  Mean = c(mean_original, mean_median, mean_fixed),
  StD = c(sd_original, sd_median, sd_fixed)
)

library(knitr)
kable(comparison_df, caption = "Comparison of Different NA Handling Methods")
```

### which method should be used? and why?

Filling missing values with the median is generally a more reliable method because it is not affected by extreme outliers and better preserves the overall structure of the data.

On the other hand, filling with a randomly selected value may work better in certain scenarios, such as large-scale datasets, but it carries the risk of distorting the natural distribution of the data.
